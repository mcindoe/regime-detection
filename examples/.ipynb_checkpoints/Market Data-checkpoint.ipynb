{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import inspect\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "import signatory\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.insert(1, '../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azran_ghahramani import delta_k_t\n",
    "from azran_ghahramani import get_maximal_eigengap_information\n",
    "from azran_ghahramani import make_W_D_from_distances\n",
    "from azran_ghahramani import multiscale_k_prototypes\n",
    "from azran_ghahramani import multiscale_k_prototypes_from_W_D\n",
    "from azran_ghahramani import write_maximal_eigengaps\n",
    "from kernels import make_laplacian_kernel\n",
    "from metrics import make_mmd_metric\n",
    "from similarities import inverse_squared\n",
    "from similarities import make_gaussian_similarity\n",
    "from similarities import make_gaussian_similarity_from_percentile\n",
    "from utils import make_colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/market_tickers.pkl', 'rb') as fp:\n",
    "    tickers = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '20000101'\n",
    "END_DATE = '20200620'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_pickle('../data/market_full_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daystrings = full_df['yearday'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for daystr in daystrings:\n",
    "    day_subset = full_df[full_df['yearday'] == daystr]\n",
    "    day_subset.dropna(inplace=True, axis='columns')\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        if ticker in day_subset.columns:\n",
    "            ticker_values = day_subset[ticker].values\n",
    "            ticker_values = ticker_values[~np.isnan(ticker_values)]\n",
    "            \n",
    "            if len(ticker_values) == 390:\n",
    "                scaled_ticker_values = ticker_values / ticker_values[0]\n",
    "\n",
    "                if daystr not in data:\n",
    "                    data[daystr] = {\n",
    "                        'points': {}\n",
    "                    }\n",
    "\n",
    "                data[daystr]['points'][ticker] = scaled_ticker_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_depth = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for daystr in data:\n",
    "    points = data[daystr]['points']\n",
    "    signatures = {}\n",
    "    logsignatures = {}\n",
    "    \n",
    "    for ticker, path in points.items():\n",
    "        augmented_path = list(enumerate(path))\n",
    "        innovation_tensor = torch.tensor(augmented_path)\n",
    "        innovation_tensor = innovation_tensor.reshape((1, len(augmented_path), 2))\n",
    "        signature = signatory.signature(innovation_tensor, depth=signature_depth)\n",
    "        logsignature = signatory.logsignature(innovation_tensor, depth=signature_depth)\n",
    "\n",
    "        signatures[ticker] = signature.tolist()[0]\n",
    "        logsignatures[ticker] = logsignature.tolist()[0]\n",
    "\n",
    "    data[daystr]['signatures'] = signatures\n",
    "    data[daystr]['logsignatures'] = logsignatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_path(path):\n",
    "    ret = [path[0]]\n",
    "    for n in range(1, len(path)):\n",
    "        this_val = path[n]\n",
    "        prev_val = path[n-1]\n",
    "        abs_diff = abs(this_val - prev_val)\n",
    "        next_pt = ret[-1] + abs_diff\n",
    "        ret.append(next_pt)\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for daystr in data:\n",
    "    points = data[daystr]['points']\n",
    "    absolute_signatures = {}\n",
    "    absolute_logsignatures = {}\n",
    "    \n",
    "    for ticker, path in points.items():\n",
    "        abs_path = absolute_path(path)\n",
    "        augmented_abs_path = list(enumerate(abs_path))\n",
    "        innovation_tensor = torch.tensor(augmented_abs_path)\n",
    "        innovation_tensor = innovation_tensor.reshape((1, len(augmented_abs_path), 2))\n",
    "        signature = signatory.signature(innovation_tensor, depth=signature_depth)\n",
    "        logsignature = signatory.logsignature(innovation_tensor, depth=signature_depth)\n",
    "\n",
    "        absolute_signatures[ticker] = signature.tolist()[0]\n",
    "        absolute_logsignatures[ticker] = logsignature.tolist()[0]\n",
    "\n",
    "    data[daystr]['absolute_signatures'] = absolute_signatures\n",
    "    data[daystr]['absolute_logsignatures'] = absolute_logsignatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_per_day = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for daystr in data:\n",
    "    day_paths = list(data[daystr]['points'].values())\n",
    "    if len(day_paths) > paths_per_day:\n",
    "        day_paths = day_paths[:paths_per_day]\n",
    "        \n",
    "    data[daystr]['plotted_paths'] = day_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotted_paths = []\n",
    "for daystr in data:\n",
    "    plotted_paths += data[daystr]['plotted_paths']\n",
    "    \n",
    "for path in plotted_paths:\n",
    "    plt.plot(path, color='purple', lw=0.25)\n",
    "    \n",
    "plt.title('Daily Price Evolution of Population')\n",
    "plt.xlabel('Minutes Elapsed')\n",
    "plt.ylabel('Relative Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wrapped_metric(metric, key, max_per_col=None):\n",
    "    def wrapped_metric(daystr1, daystr2):\n",
    "        col1 = list(data[daystr1][key].values())\n",
    "        col2 = list(data[daystr2][key].values())\n",
    "        \n",
    "        if max_per_col is not None:\n",
    "            col1 = col1[:max_per_col]\n",
    "            col2 = col2[:max_per_col]\n",
    "            \n",
    "        return metric(col1, col2)\n",
    "\n",
    "    return wrapped_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = make_laplacian_kernel(sigma=0.5)\n",
    "mmd = make_mmd_metric(kernel, kernel_repeated_arg_value=1)\n",
    "metric = make_wrapped_metric(mmd, key='logsignatures', max_per_col = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_distances(points, metric):\n",
    "    '''Get distances between all points'''\n",
    "    n_points = len(points)\n",
    "    distances = [[] for _ in range(n_points-1)]\n",
    "    for i in range(n_points-1):\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f'starting iteration {i+1} of {n_points}')\n",
    "        for j in range(i+1, n_points):\n",
    "            distances[i].append(metric(points[i], points[j]))\n",
    "            \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eigengap_values(t_values, evalues, max_clusters=None, ignore_one_clustering=False):\n",
    "    eigengap_values = {}\n",
    "    max_n_clusters = len(evalues) - 1\n",
    "        \n",
    "    if max_clusters is not None:\n",
    "        max_n_clusters = min(max_n_clusters, max_clusters)\n",
    "\n",
    "    clusters = range(1, max_n_clusters)\n",
    "    if ignore_one_clustering:\n",
    "        clusters = range(2, max_n_clusters)\n",
    "    \n",
    "    for k in clusters:\n",
    "        eigengap_values[k] = {t: delta_k_t(k-1, t, evalues) for t in t_values}\n",
    "    \n",
    "    return eigengap_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_keys = list(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate impact on distances vector for different choice of sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_vector(orig, min_vec, range_vec, skipped_indices=None):\n",
    "    assert len(orig) == len(min_vec) == len(range_vec)\n",
    "    \n",
    "    scaled_indices = range(len(orig))\n",
    "    if skipped_indices is not None:\n",
    "        scaled_indices = [n for n in range(len(orig)) if n not in skipped_indices]\n",
    "        \n",
    "    scaled_vec = []\n",
    "    for n in range(len(orig)):\n",
    "        if n in scaled_indices:\n",
    "            entry = orig[n]\n",
    "            scaled_entry = (entry - min_vec[n]) / range_vec[n]\n",
    "            scaled_vec.append(scaled_entry)\n",
    "        else:\n",
    "            scaled_vec.append(0)\n",
    "\n",
    "    return scaled_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_key(key, upper_percentile):\n",
    "    full_collection = [\n",
    "        el\n",
    "        for daystr in point_keys\n",
    "        for el in data[daystr][key].values()\n",
    "    ]\n",
    "    \n",
    "    lower_percentile = 100 - upper_percentile\n",
    "    vec_length = len(full_collection[0])\n",
    "    \n",
    "    upper_vector = [\n",
    "        np.percentile([x[n] for x in full_collection], upper_percentile)\n",
    "        for n in range(vec_length)\n",
    "    ]\n",
    "    lower_vector = [\n",
    "        np.percentile([x[n] for x in full_collection], lower_percentile)\n",
    "        for n in range(vec_length)\n",
    "    ]\n",
    "    range_vector = [\n",
    "        upper - lower \n",
    "        for upper, lower in zip(upper_vector, lower_vector)\n",
    "    ]\n",
    "    skipped_indices = [\n",
    "        idx\n",
    "        for idx, el in enumerate(range_vector)\n",
    "        if np.isclose(el, 0)\n",
    "    ]\n",
    "    \n",
    "    for daystr in data:\n",
    "        originals = data[daystr][key]\n",
    "        scaled_values = {\n",
    "            ticker: scale_vector(\n",
    "                val, lower_vector, range_vector, skipped_indices\n",
    "            ) for ticker, val in originals.items()\n",
    "        }\n",
    "        data[daystr][f'scaled_{key}'] = scaled_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_key('logsignatures', upper_percentile=95)\n",
    "scale_key('signatures', upper_percentile=95)\n",
    "scale_key('absolute_signatures', upper_percentile=95)\n",
    "scale_key('absolute_logsignatures', upper_percentile=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_W_D_from_distances(n_points, similarity, distances, self_similarity = 1):\n",
    "    # Make W matrix\n",
    "    W = np.empty((n_points, n_points))\n",
    "\n",
    "    for i in range(n_points):\n",
    "        for j in range(i, n_points):\n",
    "            if i == j:\n",
    "                W[i][j] = self_similarity\n",
    "            else:\n",
    "                distance = distances[i][j-i-1]\n",
    "                W[i][j] = similarity(distance)\n",
    "                W[j][i] = W[i][j]\n",
    "    \n",
    "    # Make D matrix from row sums\n",
    "    D = np.diag([W[i].sum() for i in range(n_points)])\n",
    "\n",
    "    # W is assumed to be full rank\n",
    "    assert np.linalg.matrix_rank(W) == len(W), 'W is not full rank'\n",
    "    return W, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrum(matr):\n",
    "    evalues, evectors = np.linalg.eig(matr)\n",
    "    evectors = evectors.T\n",
    "    evectors = np.array([evectors[n] for n in range(len(evectors))])\n",
    "\n",
    "    # Sort eigenvalues from largest to smallest; update eigenvectors\n",
    "    idx = evalues.argsort()[::-1]\n",
    "    evalues = evalues[idx]\n",
    "    evectors = evectors[idx]\n",
    "    \n",
    "    return evalues, evectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = make_laplacian_kernel(sigma=0.1)\n",
    "mmd = make_mmd_metric(kernel, kernel_repeated_arg_value=1)\n",
    "metric = make_wrapped_metric(\n",
    "    mmd,\n",
    "    key = 'scaled_absolute_logsignatures',\n",
    "    max_per_col = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = make_distances(point_keys, metric)\n",
    "flat_distances = [el for ls in distances for el in ls]\n",
    "largest_distance = max(flat_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = lambda x: 1 / x**5\n",
    "self_similarity = similarity(largest_distance)\n",
    "\n",
    "W, D = make_W_D_from_distances(\n",
    "    n_points = len(distances),\n",
    "    similarity = similarity,\n",
    "    distances = distances,\n",
    "    self_similarity = self_similarity\n",
    ")\n",
    "\n",
    "transition_matrix = np.linalg.inv(D).dot(W)\n",
    "\n",
    "evalues, evectors = get_spectrum(transition_matrix)\n",
    "\n",
    "t_values_first_segment = list(range(1, 1000))\n",
    "t_values_second_segment = np.logspace(start=3, stop=4, num=1000).tolist()\n",
    "t_values = t_values_first_segment + t_values_second_segment\n",
    "\n",
    "eigengap_values = get_eigengap_values(t_values, evalues, ignore_one_clustering=True)    \n",
    "max_egap_vals, max_attained = get_maximal_eigengap_information(eigengap_values)\n",
    "\n",
    "colours = make_colours(len(max_attained))\n",
    "\n",
    "for idx, cluster in enumerate(max_attained):\n",
    "    cluster_colour = colours[idx]\n",
    "\n",
    "    cluster_egap_separation_values = list(eigengap_values[cluster].values())\n",
    "    plt.plot(t_values, cluster_egap_separation_values, color=cluster_colour)\n",
    "    maxima_value = max_attained[cluster]['suitability']\n",
    "    maxima_location = max_attained[cluster]['n_steps']\n",
    "    plt.axvline(\n",
    "        x = maxima_location,\n",
    "        color = cluster_colour,\n",
    "        linestyle = '--',\n",
    "        label = f'{cluster} Clusters Maxima'\n",
    "    )\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.ylim(0, 1.2)\n",
    "plt.legend()\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Eigengap Separation')\n",
    "plt.title('Maximal Eigengap Separation for Number of Steps')\n",
    "\n",
    "output = multiscale_k_prototypes_from_W_D(data, W, D)\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(output, n_clusters):\n",
    "    target = [el for el in output if el['n_clusters'] == n_clusters][0]\n",
    "    return target['partition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_partition = get_partition(output, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_colours = ['red', 'green', 'blue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_colours = make_colours(len(three_partition))\n",
    "\n",
    "for cluster_idx, cluster in enumerate(three_partition):\n",
    "    cluster_colour = cluster_colours[cluster_idx]\n",
    "    for idx in cluster:\n",
    "        daystr = point_keys[idx]\n",
    "        \n",
    "        if cluster_idx == 1:\n",
    "            plotted_paths = list(data[daystr]['points'].values())\n",
    "        else:\n",
    "            plotted_paths = data[daystr]['plotted_paths']\n",
    "        \n",
    "        for path in plotted_paths:\n",
    "            plt.plot(path, color=cluster_colour, lw=0.2)\n",
    "\n",
    "    plt.ylim(0.92, 1.10)\n",
    "    plt.title(f'Market Data\\nCluster {cluster_idx+1}')\n",
    "    plt.xlabel('Minutes Elapsed')\n",
    "    plt.ylabel('Relative Price')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daystrings(indexes):\n",
    "    return [point_keys[idx] for idx in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_daystrings(three_partition[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_variation(path):\n",
    "    all_variations = []\n",
    "    for n in range(1, len(path)):\n",
    "        this_val = path[n]\n",
    "        prev_val = path[n-1]\n",
    "        difference = this_val - prev_val\n",
    "        all_variations.append(difference ** 2)\n",
    "        \n",
    "    return np.mean(all_variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, cluster in enumerate(three_partition):\n",
    "    cluster_daystrings = get_daystrings(cluster)\n",
    "    all_paths = []\n",
    "    for daystr in cluster_daystrings:\n",
    "        day_paths = list(data[daystr]['points'].values())\n",
    "        all_paths += day_paths\n",
    "        \n",
    "    final_values = [x[-1] for x in all_paths]\n",
    "    variations = [quadratic_variation(path) for path in all_paths]\n",
    "    \n",
    "    mean = np.mean(final_values)\n",
    "    average_qv = np.mean(variations)\n",
    "    \n",
    "    print()\n",
    "    print('cluster index: ', idx)\n",
    "    print('number of paths: ', len(all_paths))\n",
    "    print('average final value: ', mean)\n",
    "    print('average QV: ', average_qv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_daystrings(three_partition[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
